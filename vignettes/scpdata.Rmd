--- 
title: "The single cell proteomics data package"
author:
- name: Christophe Vanderaa
  affiliation: Computational Biology, UCLouvain
- name: Laurent Gatto
  affiliation: Computational Biology, UCLouvain
date: "`r Sys.Date()`"
output:
  BiocStyle::html_document:
    toc_float: true
vignette: >
  %\VignetteIndexEntry{scpdata}
  %\VignetteEngine{knitr::rmarkdown}
  %\usepackage[utf8]{inputenc}
---

```{r style, echo = FALSE, results = 'asis'}
BiocStyle::markdown()
```

```{r, echo = FALSE}
suppressPackageStartupMessages(library(MSnbase))
suppressPackageStartupMessages(library(ggplot2))
suppressPackageStartupMessages(library(gridExtra))
suppressPackageStartupMessages(library(reshape2))
suppressPackageStartupMessages(library(sva))
source("../R/specht2019_funs.R")
```

# Introduction 




# Specht et al. 2019

*Specht, Harrison, Emmott, Koller, Slavov. 2019. “High-Throughput Single-Cell Proteomics Quantifies the Emergence of Macrophage Heterogeneity.” bioRxiv. https://doi.org/10.1101/665307.*

The quantitative data was downloaded from the [MassIVE](https://massive.ucsd.edu/ProteoSAFe/static/massive.jsp) database with accession number *MSV000083945*. 

<!-- ### Processing the raw data into count data -->

## Data transformation 

```{r loading_data}
scp_load("../data/specht2019.rda")
sc <- specht2019
```

The `specht2019` data has already been processed to some extent. The `*.raw` files were analyzed with MaxQuant + DART-ID and the output `.txt` file was parsed into R. This output was further processed as follows:

- Only the single cell runs were kept for further analysis (experiment FP94 and FP97, no experimental information supplied)
- TMT reporter itensities (RI) were corrected for isotopic cross contamination. This is performed as follows: $cM = (C^{-1} M^T)^T$, where $M$ is the matrix containing the RI (cells $\times$ TMT reporter), and $C$ is 
$\mathbf{\color{red}{\text{no idea, this is a file loaded in the script called te269088_lot_correction.csv}}}$, (cells $\times$ cells).
- Filter out reverse hits (identified by MaxQuant), contaminants (identified by MaxQuant), and contaminated spectra (`PIF > 0.8`)
- Filter out peptides with low identification score  (`FDR >= 1%` or `PEP >= 0.02`)
- Filter out cells with less than 300 peptides
- Filter out peptides that are more than 10\% the intensity of the carrier
- Divide peptide intensities in every channel by the reference channel
- Zero or infinite intensities are replaced by `NA`'s
- Filter out cells that have a median CV larger than 0.43, for which the 30th quantile of the log10 transformed relative RIs is smaller than -2.5, or for which the median of the log10 transformed relative RI is larger than -1.3
- Divide column (cells) with median intensity and divide rows with mean intensity
- Remove rows (peptides) then columns (cells) that contain more than 99\% of missing data
- Log2 transform the data 

### Normalization 

The first step of the normalization is subtracting from each row (peptides) the mean intensity of that row. 

```{r peptide_normalization, echo = TRUE, include=TRUE}
scNorm <- row.normalize(sc)
```

Next, the rows are collapsed by proteins, meaning that measurements for peptides that belong to the same proteins are merged in a single rozw. The rows from different peptides are summarized using the median value of the peptides.

```{r peptide_aggregation}
scNorm <- aggregateByProtein(scNorm)
```

Rows than columns are finally normalized again, divi

```{r protein_cell_normalization}
scNorm <- row.normalize(scNorm)
scNorm <- col.normalize(scNorm)
```

### Imputation 

The `specht2019` data set is highly sparse it contains a majority of missing
entries. We can see this here:

```{r missing_data}
n.na <- sum(is.na(exprs(sc)))
n <- length(exprs(sc))
cat("Missing data:", round(n.na / n * 100, 2), "%")
```

Over *75%* of the data is missing ! This large proportion of missing data hinders downstream analysis and imputation is hence performed to fill in the gaps. In their paper, Specht et al. implemented the k-nearest neighbour imputation:

```{r imputation}
scImput <- imputeKNN(scNorm)
```

```{r filled_data}
n.na <- sum(is.na(exprs(scImput)))
cat("Missing data:", round(n.na / n * 100, 2), "%")
```

### Batch correction

Specht et al. performed batch correction of the single cell data using the `ComBat` function from the `sva` package. `ComBat` is a batch correction method using empirical Bayes (EB) procedure developed by Johnson et al. (2007) to remove variation linked to experimental variables (**eg** day of processing) while preserving the other sources of variation, namely the biological variation. The method is based on a previous model called the location and scale adjustment (L/S) model.

```{r batch_correction}
scBc <- batchCorrect(scImput, batch = "raw.file", target = "celltype")
```


### Quality control 

Let's check how the normalization, imputation, and batch correction affects the data. 

#### Normalization

First, let's look at the mean and standard deviation distributions before the 
normalization step:

```{r, echo = FALSE}
p1 <- ggplot(data = data.frame(mean = apply(exprs(sc), 1, mean,  na.rm = TRUE),
                         StdDev = apply(exprs(sc), 1, sd, na.rm = TRUE))) + 
  geom_point(aes(x = mean, y = StdDev), col = rgb(0, 0, 0.5, 0.5)) +
  xlab("Mean intensity") + ylab("Standard deviation") +
  ggtitle("Mean vs standard deviation \ndistribution for peptides")
p2 <- ggplot(data = data.frame(median = apply(exprs(sc), 2, median,  na.rm = TRUE),
                               StdDev = apply(exprs(sc), 2, sd, na.rm = TRUE))) + 
  geom_point(aes(x = median, y = StdDev), col = rgb(0, 0, 0.5, 0.5)) +
  ggtitle("Median vs standard deviation \ndistribution for single cells") +
  xlab("Median intensity") + ylab("Standard deviation")
grid.arrange(p1, p2, nrow = 1)
```
  
Now, after the data normalization:

```{r, echo = FALSE}
p1 <- ggplot(data = data.frame(mean = apply(exprs(scNorm), 1, mean,  na.rm = TRUE),
                         StdDev = apply(exprs(scNorm), 1, sd, na.rm = TRUE))) + 
  geom_point(aes(x = mean, y = StdDev), col = rgb(0, 0, 0.5, 0.5)) +
  ggtitle("Mean vs standard deviation \ndistribution for peptides")
p2 <- ggplot(data = data.frame(median = round(apply(exprs(scNorm), 2, median,  na.rm = TRUE), 3),
                               StdDev = apply(exprs(scNorm), 2, sd, na.rm = TRUE))) + 
  geom_point(aes(x = median, y = StdDev), col = rgb(0, 0, 0.5, 0.5)) +
  ggtitle("Median vs standard deviation \ndistribution for single cells")
grid.arrange(p1, p2, nrow = 1)
```

#### Imputation

To asses imputation, we can have a look at single cell data as a heatmap. 
Before imputation (after normalization), the data looks like this:

```{r before_imput, echo = FALSE}
show_heatmap(scNorm)
```

After imputation:

```{r after_imput, echo = FALSE}
show_heatmap(scImput)
```

Since the distance measure used for the KNN is Euclidean distance, it is interesting to look at how the similarity between cells is affected by the imputation:

```{r correl_imput, echo = FALSE}
par(mfrow = c(1,2))

R <- as.matrix( dist(t(exprs(scNorm))))
image(R, xlab = "Cell index 1", ylab = "Cell index 2", main = "Before imputation")

R <- as.matrix( dist(t(exprs(scImput))))
image(R, xlab = "Cell index 1", ylab = "Cell index 2", main = "After imputation")
```

From the two plots above we can see the imputation increases the correlation between 

### Benchmarks???

Checking the coefficient of variation (sd/mean)

```{r, echo = FALSE}
plotCV(sc)
```